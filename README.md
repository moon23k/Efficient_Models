## Efficient Models 

In general, larger model performs well for most deep learning tasks. However, there is always a trade off between performance and speed.
This repo seeks to find an efficient model by comparing the performance and speed of PLMs in the Generation Task.

<br>


## Models

**Vanilla Transformer**
This is the most basic Transformer introduced in the "Attention is All You Need" paper.

<br>

**BERT**
BERT is multiple Transformer blocks nested pretrained Encoder model.

<br>

**ALBERT**
A Lite BERT
<br>

**Distil BERT**
Distilled BERT
<br>

**Mobile BERT**

<br>

**Reformer**

<br>

**Longformer**

<br>

**BigBird**



<br><br>

## Results

<br>

## How to Use

<br>

## References
