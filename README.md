## Efficient Models 

In general, larger model performs well for most deep learning tasks. However, there is always a trade off between performance and speed.
This repo seeks to find an efficient model by comparing the performance and speed of PLMs in the Generation Task.

<br>


## Models

**Vanilla Transformer**<br>
This is the most basic Transformer introduced in the "Attention is All You Need" paper.

<br>

**BERT**<br>
BERT is multiple Transformer blocks nested pretrained Encoder model.

<br>

**ALBERT**<br>
A Lite BERT
<br>

**Distil BERT**<br>
Distilled BERT
<br>

**Mobile BERT**<br>

<br>

**Reformer**<br>

<br>

**Longformer**

<br>

**BigBird**



<br><br>

## Results

<br>

## How to Use

<br>

## References
