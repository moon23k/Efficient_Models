## Recursive_Transformer

This repo contains a series of experiments to evaluate how well a Transformer can handle long sentences when given recuurent properties. In the experiment, Vanilla Transformer is set as a reference point and evaluate how much better or worse the two other recurrent models perform in a longer text.

<br>

## Model desc

### Vanilla Transformer
> This model is the basic version of Transformer Architecture. It showed good performance by breaking through the limitations of the existing RNN, and is the basis of most sota models.

<br>

### Universal Transformer
> Universal Transformer is designed to improve the problem that the existing Transformer does not handle long sentences well. To solve this problem Universal Transformer recurrently uses the blocks of the Vanilla Transformer.

<br>

### Sliced Recurrent Transformer
> TBD
<br>


## Configs

### Model Configs

<br>

### Training Configs

<br>

## Results

<br>

## How to Use

<br>

## References
Attention is all you need
Universal Transformer
Sliced Recurrent Transformer
